# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#
logger:
      classname: bbrl.utils.logger.TFLogger
      log_dir: ./sac_logs/
      verbose: False
      every_n_seconds: 10

scenario:
      classname: bbrl_subspaces.frameworks.GymScenario
      domain: PendulumSubspace-v1
      repeat_scenario: 1
      tasks: ["normal"]
      base_env:
            env_name: ${scenario.domain}
            n_envs: 8
            nb_evals: 500
            n_rollouts: ${framework.alpha_search.n_rollouts}
            seed:
                  train: 1
                  eval: 99
                  alpha_search: ${framework.alpha_search.seed}

framework:
      classname: bbrl_subspaces.frameworks.Subspace
      seed: 0
      train_algorithm:
            classname: bbrl_subspaces.algorithms.SAC
            name: "SAC"
            env_name: ${scenario.base_env.env_name}
            params:
                  save_best: False
                  save_reward_curves: True
                  algorithm:
                        seed:
                              q: 123
                              explorer: 456
                              torch: 789
                        n_envs: 8
                        n_steps_train: 32
                        n_steps: 1_000_000
                        eval_interval: 5000
                        nb_evals: 10
                        buffer_size: 1e6
                        batch_size: 256
                        learning_starts: 10_000
                        tau_target: 0.05
                        max_grad_norm: 0.5
                        discount_factor: 0.95
                        entropy_mode: "auto" # "auto" or "fixed"
                        init_entropy_coef: 2e-7
                        avg_reward_interval: 5000
                        n_samples: ${scenario.base_env.nb_evals}
                        n_estimations: ${framework.alpha_search.n_samples}
                        anticollapse_coef: 1.0

                  actor_optimizer:
                        classname: torch.optim.Adam
                        lr: 1e-3

                  critic_optimizer:
                        classname: torch.optim.Adam
                        lr: 1e-3

                  entropy_coef_optimizer:
                        classname: torch.optim.Adam
                        lr: 1e-3

                  policy_agent:
                        classname: bbrl_subspaces.agents.SubspaceActionAgent
                        hidden_size: 32
                        input_dimension: nil
                        output_dimension: nil
                        n_initial_anchors: 3
                        start_steps: 0
                        dist_type: flat
                        refresh_rate: 1.
                        resampling_policy: False
                        repeat_alpha: 100

                  critic_agent:
                        classname: bbrl_subspaces.agents.AlphaCritic
                        hidden_size: 256
                        obs_dimension: nil
                        action_dimension: nil
                        n_anchors: ${framework.train_algorithm.params.policy_agent.n_initial_anchors}

      alpha_search:
            classname: bbrl_subspaces.algorithms.AlphaSearch
            n_rollouts: 32
            n_samples: 1024
            n_validation_steps: 200
            seed: 0
            prune_subspace: False

      evaluation:
            n_rollouts: 1
            oracle_rollouts: 1

      visualization:
            classname: bbrl_subspaces.visualization.SubspaceVisualizer
            algorithm_name: ${framework.train_algorithm.name}
            env_name: ${scenario.base_env.env_name}
            num_points: 40
            interactive: False
            thresholds: []